{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch  \n",
    "import gym\n",
    "import numpy as np  \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# hyperparameters\n",
    "hidden_size = 256\n",
    "learning_rate = 3e-4\n",
    "\n",
    "# Constants\n",
    "GAMMA = 0.99\n",
    "num_steps = 300\n",
    "max_episodes = 3000\n",
    "epsilon = 0.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_size, learning_rate=3e-4):\n",
    "        super(ActorCritic, self).__init__()\n",
    "\n",
    "        self.num_actions = num_actions\n",
    "        self.critic_linear1 = nn.Linear(num_inputs, hidden_size)\n",
    "        self.critic_linear2 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "        self.actor_linear1 = nn.Linear(num_inputs, hidden_size)\n",
    "        self.actor_linear2 = nn.Linear(hidden_size, num_actions)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        state = Variable(torch.from_numpy(state).float().unsqueeze(0))\n",
    "        \n",
    "        #g3t the value\n",
    "        value = F.relu(self.critic_linear1(state))\n",
    "        value = self.critic_linear2(value)\n",
    "        \n",
    "        #get the new policy dist\n",
    "        p_d = F.relu(self.actor_linear1(state))\n",
    "        p_d = F.softmax(self.actor_linear2(p_d))\n",
    "        \n",
    "        return value, p_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/advaiths/.local/lib/python3.6/site-packages/ipykernel_launcher.py:21: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0, reward: 14.0, total length: 13, average length: 13.0 \n",
      "episode: 10, reward: 50.0, total length: 49, average length: 20.4 \n",
      "episode: 20, reward: 17.0, total length: 16, average length: 21.5 \n",
      "episode: 30, reward: 32.0, total length: 31, average length: 18.6 \n",
      "episode: 40, reward: 31.0, total length: 30, average length: 18.3 \n",
      "episode: 50, reward: 10.0, total length: 9, average length: 19.1 \n",
      "episode: 60, reward: 39.0, total length: 38, average length: 25.8 \n",
      "episode: 70, reward: 25.0, total length: 24, average length: 22.1 \n",
      "episode: 80, reward: 12.0, total length: 11, average length: 23.8 \n",
      "episode: 90, reward: 21.0, total length: 20, average length: 26.5 \n",
      "episode: 100, reward: 9.0, total length: 8, average length: 20.2 \n",
      "episode: 110, reward: 21.0, total length: 20, average length: 25.7 \n",
      "episode: 120, reward: 13.0, total length: 12, average length: 23.5 \n",
      "episode: 130, reward: 42.0, total length: 41, average length: 27.4 \n",
      "episode: 140, reward: 13.0, total length: 12, average length: 22.5 \n",
      "episode: 150, reward: 26.0, total length: 25, average length: 20.5 \n",
      "episode: 160, reward: 21.0, total length: 20, average length: 29.7 \n",
      "episode: 170, reward: 13.0, total length: 12, average length: 22.6 \n",
      "episode: 180, reward: 19.0, total length: 18, average length: 36.0 \n",
      "episode: 190, reward: 13.0, total length: 12, average length: 23.2 \n",
      "episode: 200, reward: 21.0, total length: 20, average length: 30.6 \n",
      "episode: 210, reward: 13.0, total length: 12, average length: 19.7 \n",
      "episode: 220, reward: 18.0, total length: 17, average length: 36.7 \n",
      "episode: 230, reward: 30.0, total length: 29, average length: 30.1 \n",
      "episode: 240, reward: 73.0, total length: 72, average length: 29.0 \n",
      "episode: 250, reward: 32.0, total length: 31, average length: 27.4 \n",
      "episode: 260, reward: 67.0, total length: 66, average length: 36.5 \n",
      "episode: 270, reward: 86.0, total length: 85, average length: 38.8 \n",
      "episode: 280, reward: 13.0, total length: 12, average length: 33.8 \n",
      "episode: 290, reward: 29.0, total length: 28, average length: 34.9 \n",
      "episode: 300, reward: 24.0, total length: 23, average length: 36.8 \n",
      "episode: 310, reward: 40.0, total length: 39, average length: 31.7 \n",
      "episode: 320, reward: 28.0, total length: 27, average length: 39.4 \n",
      "episode: 330, reward: 40.0, total length: 39, average length: 33.7 \n",
      "episode: 340, reward: 24.0, total length: 23, average length: 35.3 \n",
      "episode: 350, reward: 34.0, total length: 33, average length: 31.7 \n",
      "episode: 360, reward: 63.0, total length: 62, average length: 39.0 \n",
      "episode: 370, reward: 17.0, total length: 16, average length: 40.1 \n",
      "episode: 380, reward: 18.0, total length: 17, average length: 50.9 \n",
      "episode: 390, reward: 13.0, total length: 12, average length: 39.3 \n",
      "episode: 400, reward: 29.0, total length: 28, average length: 41.9 \n",
      "episode: 410, reward: 54.0, total length: 53, average length: 40.6 \n",
      "episode: 420, reward: 65.0, total length: 64, average length: 51.8 \n",
      "episode: 430, reward: 48.0, total length: 47, average length: 37.1 \n",
      "episode: 440, reward: 32.0, total length: 31, average length: 34.1 \n",
      "episode: 450, reward: 94.0, total length: 93, average length: 46.7 \n",
      "episode: 460, reward: 50.0, total length: 49, average length: 50.8 \n",
      "episode: 470, reward: 53.0, total length: 52, average length: 47.0 \n",
      "episode: 480, reward: 56.0, total length: 55, average length: 50.4 \n",
      "episode: 490, reward: 76.0, total length: 75, average length: 37.2 \n",
      "episode: 500, reward: 108.0, total length: 107, average length: 49.1 \n",
      "episode: 510, reward: 45.0, total length: 44, average length: 33.9 \n",
      "episode: 520, reward: 19.0, total length: 18, average length: 46.0 \n",
      "episode: 530, reward: 68.0, total length: 67, average length: 48.7 \n",
      "episode: 540, reward: 26.0, total length: 25, average length: 38.8 \n",
      "episode: 550, reward: 24.0, total length: 23, average length: 32.4 \n",
      "episode: 560, reward: 50.0, total length: 49, average length: 47.8 \n",
      "episode: 570, reward: 21.0, total length: 20, average length: 46.7 \n",
      "episode: 580, reward: 30.0, total length: 29, average length: 41.7 \n",
      "episode: 590, reward: 18.0, total length: 17, average length: 42.9 \n",
      "episode: 600, reward: 41.0, total length: 40, average length: 51.9 \n",
      "episode: 610, reward: 129.0, total length: 128, average length: 52.1 \n",
      "episode: 620, reward: 14.0, total length: 13, average length: 37.6 \n",
      "episode: 630, reward: 62.0, total length: 61, average length: 50.7 \n",
      "episode: 640, reward: 38.0, total length: 37, average length: 37.8 \n",
      "episode: 650, reward: 59.0, total length: 58, average length: 66.1 \n",
      "episode: 660, reward: 69.0, total length: 68, average length: 43.0 \n",
      "episode: 670, reward: 93.0, total length: 92, average length: 64.9 \n",
      "episode: 680, reward: 22.0, total length: 21, average length: 61.6 \n",
      "episode: 690, reward: 51.0, total length: 50, average length: 62.5 \n",
      "episode: 700, reward: 40.0, total length: 39, average length: 44.1 \n",
      "episode: 710, reward: 62.0, total length: 61, average length: 59.5 \n",
      "episode: 720, reward: 45.0, total length: 44, average length: 47.1 \n",
      "episode: 730, reward: 90.0, total length: 89, average length: 51.4 \n",
      "episode: 740, reward: 35.0, total length: 34, average length: 82.3 \n",
      "episode: 750, reward: 34.0, total length: 33, average length: 63.6 \n",
      "episode: 760, reward: 17.0, total length: 16, average length: 73.8 \n",
      "episode: 770, reward: 32.0, total length: 31, average length: 91.5 \n",
      "episode: 780, reward: 63.0, total length: 62, average length: 66.4 \n",
      "episode: 790, reward: 127.0, total length: 126, average length: 89.9 \n",
      "episode: 800, reward: 116.0, total length: 115, average length: 89.4 \n",
      "episode: 810, reward: 17.0, total length: 16, average length: 51.0 \n",
      "episode: 820, reward: 32.0, total length: 31, average length: 84.2 \n",
      "episode: 830, reward: 46.0, total length: 45, average length: 56.0 \n",
      "episode: 840, reward: 66.0, total length: 65, average length: 79.6 \n",
      "episode: 850, reward: 32.0, total length: 31, average length: 55.6 \n",
      "episode: 860, reward: 129.0, total length: 128, average length: 82.4 \n",
      "episode: 870, reward: 103.0, total length: 102, average length: 85.9 \n",
      "episode: 880, reward: 200.0, total length: 199, average length: 75.4 \n",
      "episode: 890, reward: 61.0, total length: 60, average length: 79.4 \n",
      "episode: 900, reward: 38.0, total length: 37, average length: 56.3 \n",
      "episode: 910, reward: 131.0, total length: 130, average length: 75.6 \n",
      "episode: 920, reward: 30.0, total length: 29, average length: 75.4 \n",
      "episode: 930, reward: 138.0, total length: 137, average length: 69.7 \n",
      "episode: 940, reward: 107.0, total length: 106, average length: 50.0 \n",
      "episode: 950, reward: 179.0, total length: 178, average length: 121.7 \n",
      "episode: 960, reward: 163.0, total length: 162, average length: 102.6 \n",
      "episode: 970, reward: 38.0, total length: 37, average length: 48.4 \n",
      "episode: 980, reward: 57.0, total length: 56, average length: 148.2 \n",
      "episode: 990, reward: 93.0, total length: 92, average length: 93.1 \n",
      "episode: 1000, reward: 20.0, total length: 19, average length: 90.5 \n",
      "episode: 1010, reward: 66.0, total length: 65, average length: 95.9 \n",
      "episode: 1020, reward: 70.0, total length: 69, average length: 69.0 \n",
      "episode: 1030, reward: 50.0, total length: 49, average length: 110.8 \n",
      "episode: 1040, reward: 30.0, total length: 29, average length: 69.8 \n",
      "episode: 1050, reward: 131.0, total length: 130, average length: 88.5 \n",
      "episode: 1060, reward: 150.0, total length: 149, average length: 112.0 \n",
      "episode: 1070, reward: 122.0, total length: 121, average length: 94.5 \n",
      "episode: 1080, reward: 15.0, total length: 14, average length: 99.5 \n",
      "episode: 1090, reward: 147.0, total length: 146, average length: 98.9 \n",
      "episode: 1100, reward: 82.0, total length: 81, average length: 90.5 \n",
      "episode: 1110, reward: 200.0, total length: 199, average length: 152.8 \n",
      "episode: 1120, reward: 200.0, total length: 199, average length: 82.2 \n",
      "episode: 1130, reward: 90.0, total length: 89, average length: 93.3 \n",
      "episode: 1140, reward: 126.0, total length: 125, average length: 110.3 \n",
      "episode: 1150, reward: 139.0, total length: 138, average length: 106.3 \n",
      "episode: 1160, reward: 200.0, total length: 199, average length: 127.6 \n",
      "episode: 1170, reward: 101.0, total length: 100, average length: 88.7 \n",
      "episode: 1180, reward: 196.0, total length: 195, average length: 139.8 \n",
      "episode: 1190, reward: 80.0, total length: 79, average length: 108.5 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1200, reward: 136.0, total length: 135, average length: 132.0 \n",
      "episode: 1210, reward: 200.0, total length: 199, average length: 146.7 \n",
      "episode: 1220, reward: 152.0, total length: 151, average length: 139.5 \n",
      "episode: 1230, reward: 110.0, total length: 109, average length: 154.2 \n",
      "episode: 1240, reward: 166.0, total length: 165, average length: 154.4 \n",
      "episode: 1250, reward: 200.0, total length: 199, average length: 135.7 \n",
      "episode: 1260, reward: 25.0, total length: 24, average length: 143.4 \n",
      "episode: 1270, reward: 145.0, total length: 144, average length: 130.5 \n",
      "episode: 1280, reward: 187.0, total length: 186, average length: 108.5 \n",
      "episode: 1290, reward: 33.0, total length: 32, average length: 137.9 \n",
      "episode: 1300, reward: 177.0, total length: 176, average length: 147.2 \n",
      "episode: 1310, reward: 54.0, total length: 53, average length: 83.3 \n",
      "episode: 1320, reward: 115.0, total length: 114, average length: 83.3 \n",
      "episode: 1330, reward: 103.0, total length: 102, average length: 102.0 \n",
      "episode: 1340, reward: 104.0, total length: 103, average length: 102.9 \n",
      "episode: 1350, reward: 29.0, total length: 28, average length: 119.4 \n",
      "episode: 1360, reward: 70.0, total length: 69, average length: 119.5 \n",
      "episode: 1370, reward: 200.0, total length: 199, average length: 152.7 \n",
      "episode: 1380, reward: 166.0, total length: 165, average length: 116.4 \n",
      "episode: 1390, reward: 48.0, total length: 47, average length: 93.9 \n",
      "episode: 1400, reward: 200.0, total length: 199, average length: 113.9 \n",
      "episode: 1410, reward: 200.0, total length: 199, average length: 133.9 \n",
      "episode: 1420, reward: 25.0, total length: 24, average length: 130.3 \n",
      "episode: 1430, reward: 128.0, total length: 127, average length: 128.6 \n",
      "episode: 1440, reward: 106.0, total length: 105, average length: 126.5 \n",
      "episode: 1450, reward: 172.0, total length: 171, average length: 133.2 \n",
      "episode: 1460, reward: 36.0, total length: 35, average length: 123.4 \n",
      "episode: 1470, reward: 200.0, total length: 199, average length: 118.6 \n",
      "episode: 1480, reward: 34.0, total length: 33, average length: 81.6 \n",
      "episode: 1490, reward: 69.0, total length: 68, average length: 144.4 \n",
      "episode: 1500, reward: 161.0, total length: 160, average length: 133.4 \n",
      "episode: 1510, reward: 165.0, total length: 164, average length: 97.6 \n",
      "episode: 1520, reward: 200.0, total length: 199, average length: 90.2 \n",
      "episode: 1530, reward: 51.0, total length: 50, average length: 91.8 \n",
      "episode: 1540, reward: 176.0, total length: 175, average length: 127.0 \n",
      "episode: 1550, reward: 132.0, total length: 131, average length: 100.4 \n",
      "episode: 1560, reward: 200.0, total length: 199, average length: 144.2 \n",
      "episode: 1570, reward: 200.0, total length: 199, average length: 103.4 \n",
      "episode: 1580, reward: 200.0, total length: 199, average length: 127.5 \n",
      "episode: 1590, reward: 159.0, total length: 158, average length: 140.3 \n",
      "episode: 1600, reward: 190.0, total length: 189, average length: 149.3 \n",
      "episode: 1610, reward: 160.0, total length: 159, average length: 149.1 \n",
      "episode: 1620, reward: 147.0, total length: 146, average length: 124.1 \n",
      "episode: 1630, reward: 81.0, total length: 80, average length: 124.9 \n",
      "episode: 1640, reward: 94.0, total length: 93, average length: 142.3 \n",
      "episode: 1650, reward: 134.0, total length: 133, average length: 111.5 \n",
      "episode: 1660, reward: 69.0, total length: 68, average length: 117.5 \n",
      "episode: 1670, reward: 183.0, total length: 182, average length: 112.4 \n",
      "episode: 1680, reward: 98.0, total length: 97, average length: 116.6 \n",
      "episode: 1690, reward: 22.0, total length: 21, average length: 111.1 \n",
      "episode: 1700, reward: 122.0, total length: 121, average length: 109.9 \n",
      "episode: 1710, reward: 144.0, total length: 143, average length: 174.0 \n",
      "episode: 1720, reward: 192.0, total length: 191, average length: 163.3 \n",
      "episode: 1730, reward: 163.0, total length: 162, average length: 127.2 \n",
      "episode: 1740, reward: 124.0, total length: 123, average length: 153.5 \n",
      "episode: 1750, reward: 143.0, total length: 142, average length: 146.6 \n",
      "episode: 1760, reward: 200.0, total length: 199, average length: 138.3 \n",
      "episode: 1770, reward: 177.0, total length: 176, average length: 143.3 \n",
      "episode: 1780, reward: 200.0, total length: 199, average length: 154.9 \n",
      "episode: 1790, reward: 109.0, total length: 108, average length: 141.1 \n",
      "episode: 1800, reward: 200.0, total length: 199, average length: 146.6 \n",
      "episode: 1810, reward: 43.0, total length: 42, average length: 143.9 \n",
      "episode: 1820, reward: 70.0, total length: 69, average length: 115.2 \n",
      "episode: 1830, reward: 200.0, total length: 199, average length: 109.5 \n",
      "episode: 1840, reward: 193.0, total length: 192, average length: 137.2 \n",
      "episode: 1850, reward: 30.0, total length: 29, average length: 116.5 \n",
      "episode: 1860, reward: 119.0, total length: 118, average length: 120.6 \n",
      "episode: 1870, reward: 37.0, total length: 36, average length: 118.8 \n",
      "episode: 1880, reward: 184.0, total length: 183, average length: 125.5 \n",
      "episode: 1890, reward: 148.0, total length: 147, average length: 173.8 \n",
      "episode: 1900, reward: 164.0, total length: 163, average length: 128.6 \n",
      "episode: 1910, reward: 141.0, total length: 140, average length: 129.1 \n",
      "episode: 1920, reward: 78.0, total length: 77, average length: 140.0 \n",
      "episode: 1930, reward: 200.0, total length: 199, average length: 154.2 \n",
      "episode: 1940, reward: 25.0, total length: 24, average length: 128.7 \n",
      "episode: 1950, reward: 116.0, total length: 115, average length: 122.0 \n",
      "episode: 1960, reward: 181.0, total length: 180, average length: 112.1 \n",
      "episode: 1970, reward: 135.0, total length: 134, average length: 132.4 \n",
      "episode: 1980, reward: 113.0, total length: 112, average length: 147.4 \n",
      "episode: 1990, reward: 20.0, total length: 19, average length: 120.5 \n",
      "episode: 2000, reward: 200.0, total length: 199, average length: 121.8 \n",
      "episode: 2010, reward: 200.0, total length: 199, average length: 113.8 \n",
      "episode: 2020, reward: 103.0, total length: 102, average length: 139.7 \n",
      "episode: 2030, reward: 200.0, total length: 199, average length: 119.0 \n",
      "episode: 2040, reward: 120.0, total length: 119, average length: 115.1 \n",
      "episode: 2050, reward: 200.0, total length: 199, average length: 115.2 \n",
      "episode: 2060, reward: 200.0, total length: 199, average length: 112.0 \n",
      "episode: 2070, reward: 200.0, total length: 199, average length: 133.9 \n",
      "episode: 2080, reward: 150.0, total length: 149, average length: 144.0 \n",
      "episode: 2090, reward: 122.0, total length: 121, average length: 141.2 \n",
      "episode: 2100, reward: 200.0, total length: 199, average length: 141.1 \n",
      "episode: 2110, reward: 92.0, total length: 91, average length: 93.3 \n",
      "episode: 2120, reward: 147.0, total length: 146, average length: 103.4 \n",
      "episode: 2130, reward: 165.0, total length: 164, average length: 142.8 \n",
      "episode: 2140, reward: 160.0, total length: 159, average length: 123.8 \n",
      "episode: 2150, reward: 200.0, total length: 199, average length: 154.0 \n",
      "episode: 2160, reward: 132.0, total length: 131, average length: 146.7 \n",
      "episode: 2170, reward: 200.0, total length: 199, average length: 157.8 \n",
      "episode: 2180, reward: 200.0, total length: 199, average length: 124.3 \n",
      "episode: 2190, reward: 200.0, total length: 199, average length: 127.6 \n",
      "episode: 2200, reward: 133.0, total length: 132, average length: 126.9 \n",
      "episode: 2210, reward: 200.0, total length: 199, average length: 154.6 \n",
      "episode: 2220, reward: 95.0, total length: 94, average length: 154.8 \n",
      "episode: 2230, reward: 172.0, total length: 171, average length: 171.4 \n",
      "episode: 2240, reward: 200.0, total length: 199, average length: 153.4 \n",
      "episode: 2250, reward: 72.0, total length: 71, average length: 117.3 \n",
      "episode: 2260, reward: 101.0, total length: 100, average length: 111.9 \n",
      "episode: 2270, reward: 200.0, total length: 199, average length: 134.1 \n",
      "episode: 2280, reward: 123.0, total length: 122, average length: 125.1 \n",
      "episode: 2290, reward: 68.0, total length: 67, average length: 115.3 \n",
      "episode: 2300, reward: 134.0, total length: 133, average length: 157.5 \n",
      "episode: 2310, reward: 128.0, total length: 127, average length: 156.7 \n",
      "episode: 2320, reward: 158.0, total length: 157, average length: 119.4 \n",
      "episode: 2330, reward: 94.0, total length: 93, average length: 152.2 \n",
      "episode: 2340, reward: 170.0, total length: 169, average length: 157.8 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2350, reward: 165.0, total length: 164, average length: 174.9 \n",
      "episode: 2360, reward: 161.0, total length: 160, average length: 109.1 \n",
      "episode: 2370, reward: 160.0, total length: 159, average length: 147.5 \n",
      "episode: 2380, reward: 121.0, total length: 120, average length: 148.3 \n",
      "episode: 2390, reward: 130.0, total length: 129, average length: 170.9 \n",
      "episode: 2400, reward: 172.0, total length: 171, average length: 136.1 \n",
      "episode: 2410, reward: 175.0, total length: 174, average length: 168.4 \n",
      "episode: 2420, reward: 168.0, total length: 167, average length: 157.7 \n",
      "episode: 2430, reward: 44.0, total length: 43, average length: 159.5 \n",
      "episode: 2440, reward: 39.0, total length: 38, average length: 128.5 \n",
      "episode: 2450, reward: 98.0, total length: 97, average length: 129.7 \n",
      "episode: 2460, reward: 46.0, total length: 45, average length: 145.8 \n",
      "episode: 2470, reward: 89.0, total length: 88, average length: 170.6 \n",
      "episode: 2480, reward: 31.0, total length: 30, average length: 136.6 \n",
      "episode: 2490, reward: 180.0, total length: 179, average length: 158.7 \n",
      "episode: 2500, reward: 133.0, total length: 132, average length: 153.8 \n",
      "episode: 2510, reward: 198.0, total length: 197, average length: 118.3 \n",
      "episode: 2520, reward: 200.0, total length: 199, average length: 145.2 \n",
      "episode: 2530, reward: 76.0, total length: 75, average length: 115.4 \n",
      "episode: 2540, reward: 200.0, total length: 199, average length: 135.5 \n",
      "episode: 2550, reward: 155.0, total length: 154, average length: 146.4 \n",
      "episode: 2560, reward: 41.0, total length: 40, average length: 116.6 \n",
      "episode: 2570, reward: 153.0, total length: 152, average length: 135.8 \n",
      "episode: 2580, reward: 200.0, total length: 199, average length: 177.3 \n",
      "episode: 2590, reward: 200.0, total length: 199, average length: 157.4 \n",
      "episode: 2600, reward: 135.0, total length: 134, average length: 132.6 \n",
      "episode: 2610, reward: 111.0, total length: 110, average length: 147.1 \n",
      "episode: 2620, reward: 200.0, total length: 199, average length: 174.6 \n",
      "episode: 2630, reward: 200.0, total length: 199, average length: 117.7 \n",
      "episode: 2640, reward: 124.0, total length: 123, average length: 143.6 \n",
      "episode: 2650, reward: 200.0, total length: 199, average length: 171.9 \n",
      "episode: 2660, reward: 200.0, total length: 199, average length: 124.8 \n",
      "episode: 2670, reward: 170.0, total length: 169, average length: 117.7 \n",
      "episode: 2680, reward: 200.0, total length: 199, average length: 170.9 \n",
      "episode: 2690, reward: 123.0, total length: 122, average length: 111.2 \n",
      "episode: 2700, reward: 200.0, total length: 199, average length: 132.8 \n",
      "episode: 2710, reward: 139.0, total length: 138, average length: 133.6 \n",
      "episode: 2720, reward: 183.0, total length: 182, average length: 152.8 \n",
      "episode: 2730, reward: 200.0, total length: 199, average length: 147.7 \n",
      "episode: 2740, reward: 192.0, total length: 191, average length: 185.4 \n",
      "episode: 2750, reward: 130.0, total length: 129, average length: 139.0 \n",
      "episode: 2760, reward: 200.0, total length: 199, average length: 167.0 \n",
      "episode: 2770, reward: 200.0, total length: 199, average length: 181.1 \n",
      "episode: 2780, reward: 112.0, total length: 111, average length: 146.8 \n",
      "episode: 2790, reward: 50.0, total length: 49, average length: 158.7 \n",
      "episode: 2800, reward: 200.0, total length: 199, average length: 148.1 \n",
      "episode: 2810, reward: 200.0, total length: 199, average length: 168.4 \n",
      "episode: 2820, reward: 13.0, total length: 12, average length: 153.5 \n",
      "episode: 2830, reward: 150.0, total length: 149, average length: 165.2 \n",
      "episode: 2840, reward: 169.0, total length: 168, average length: 132.5 \n",
      "episode: 2850, reward: 183.0, total length: 182, average length: 141.5 \n",
      "episode: 2860, reward: 169.0, total length: 168, average length: 154.6 \n",
      "episode: 2870, reward: 179.0, total length: 178, average length: 128.3 \n",
      "episode: 2880, reward: 200.0, total length: 199, average length: 143.4 \n",
      "episode: 2890, reward: 179.0, total length: 178, average length: 145.8 \n",
      "episode: 2900, reward: 88.0, total length: 87, average length: 130.1 \n",
      "episode: 2910, reward: 200.0, total length: 199, average length: 181.0 \n",
      "episode: 2920, reward: 200.0, total length: 199, average length: 145.4 \n",
      "episode: 2930, reward: 130.0, total length: 129, average length: 125.0 \n",
      "episode: 2940, reward: 181.0, total length: 180, average length: 170.4 \n",
      "episode: 2950, reward: 165.0, total length: 164, average length: 128.3 \n",
      "episode: 2960, reward: 65.0, total length: 64, average length: 169.8 \n",
      "episode: 2970, reward: 171.0, total length: 170, average length: 148.8 \n",
      "episode: 2980, reward: 200.0, total length: 199, average length: 181.5 \n",
      "episode: 2990, reward: 99.0, total length: 98, average length: 117.4 \n"
     ]
    }
   ],
   "source": [
    "num_inputs = env.observation_space.shape[0]\n",
    "num_outputs = env.action_space.n\n",
    "ac = ActorCritic(num_inputs, num_outputs, hidden_size)\n",
    "def train_actor_agent(env):\n",
    "    \n",
    "    \n",
    "    \n",
    "    ac_opt = optim.Adam(ac.parameters(), lr=learning_rate)\n",
    "    \n",
    "    all_lengths = []\n",
    "    average_lengths = []\n",
    "    all_rewards = []\n",
    "    entropy_term = 0\n",
    "    \n",
    "    for episode in range(max_episodes):\n",
    "        log_probs = []\n",
    "        values = []\n",
    "        rewards = []\n",
    "\n",
    "        state = env.reset()\n",
    "        for steps in range(num_steps):\n",
    "            value, p_d = ac.forward(state)\n",
    "            value = value.detach().numpy()[0,0]\n",
    "            dist = p_d.detach().numpy()\n",
    "            \n",
    "            #epsilon greedy action choice\n",
    "            exp = np.random.uniform(0, 1)\n",
    "            if exp >= epsilon:\n",
    "                action = np.random.choice(num_outputs, p=np.squeeze(dist))\n",
    "            else:\n",
    "                action = np.random.choice(num_outputs)\n",
    "            \n",
    "            log_prob = torch.log(p_d.squeeze(0)[action])\n",
    "            entropy = -np.sum(np.mean(dist) * np.log(dist))\n",
    "            new_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            rewards.append(reward)\n",
    "            values.append(value)\n",
    "            log_probs.append(log_prob)\n",
    "            entropy_term += entropy\n",
    "            state = new_state\n",
    "            \n",
    "            \n",
    "            if done or steps == num_steps-1:\n",
    "                Qval, _ = ac.forward(new_state)\n",
    "                Qval = Qval.detach().numpy()[0,0]\n",
    "                all_rewards.append(np.sum(rewards))\n",
    "                all_lengths.append(steps)\n",
    "                average_lengths.append(np.mean(all_lengths[-10:]))\n",
    "                if episode % 10 == 0:                    \n",
    "                    sys.stdout.write(\"episode: {}, reward: {}, total length: {}, average length: {} \\n\".format(episode, np.sum(rewards), steps, average_lengths[-1]))\n",
    "                break\n",
    "            \n",
    "        Qvals = np.zeros_like(values)\n",
    "\n",
    "        #iterate Q values\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            Qval = rewards[t] + GAMMA * Qval\n",
    "            Qvals[t] = Qval\n",
    "\n",
    "\n",
    "        values = torch.FloatTensor(values)\n",
    "        Qvals = torch.FloatTensor(Qvals)\n",
    "        log_probs = torch.stack(log_probs)\n",
    "\n",
    "        advantage = Qvals - values\n",
    "        actor_loss = (-log_probs * advantage).mean()\n",
    "        critic_loss = .5*advantage.pow(2).mean()\n",
    "        ac_loss = actor_loss + critic_loss + .001*entropy_term\n",
    "\n",
    "        ac_opt.zero_grad()\n",
    "        ac_loss.backward()\n",
    "        ac_opt.step()\n",
    "    \n",
    "\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "train_actor_agent(env)\n",
    "torch.save(ac.state_dict(), \"./ac.weights\")\n",
    "            \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/advaiths/.local/lib/python3.6/site-packages/ipykernel_launcher.py:21: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0, reward: 200.0, total length: 199\n",
      "episode: 10, reward: 1880.0, total length: 47\n",
      "episode: 20, reward: 3326.0, total length: 150\n",
      "episode: 30, reward: 4882.0, total length: 157\n",
      "episode: 40, reward: 6254.0, total length: 175\n",
      "episode: 50, reward: 7959.0, total length: 171\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-2cac95375727>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mrewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0msteps\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gym/envs/classic_control/cartpole.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoletrans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_rotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_rgb_array\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, return_rgb_array)\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monetime_geoms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_rgb_array\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misopen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pyglet/window/xlib/__init__.py\u001b[0m in \u001b[0;36mflip\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    487\u001b[0m         \u001b[0;31m# TODO canvas.flip?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sync_resize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pyglet/gl/xlib.py\u001b[0m in \u001b[0;36mflip\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vsync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_vsync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m         \u001b[0mglx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglXSwapBuffers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_display\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglx_window\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "rewards = []\n",
    "ac = ActorCritic(num_inputs, num_outputs, hidden_size)\n",
    "ac.load_state_dict(torch.load(\"./ac_weights\"))\n",
    "ac.eval()\n",
    "for episode in range(max_episodes):\n",
    "    state = env.reset()\n",
    "    for steps in range(num_steps):\n",
    "        value, p_d = ac.forward(state)\n",
    "        value = value.detach().numpy()[0,0]\n",
    "        dist = p_d.detach().numpy()\n",
    "\n",
    "        #epsilon greedy action choice\n",
    "        exp = np.random.uniform(0, 1)\n",
    "        if exp >= epsilon:\n",
    "            action = np.random.choice(num_outputs, p=np.squeeze(dist))\n",
    "        else:\n",
    "            action = np.random.choice(num_outputs)\n",
    "\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        rewards.append(reward)\n",
    "        state = new_state\n",
    "        env.render()\n",
    "        if done or steps == num_steps-1:\n",
    "            if episode % 10 == 0:                    \n",
    "                sys.stdout.write(\"episode: {}, reward: {}, total length: {}\\n\".format(episode, np.sum(rewards), steps))\n",
    "            break\n",
    "        \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
